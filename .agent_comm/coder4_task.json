{
  "agent_id": "coder4",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.CV_2508.10893v1_STream3R_Scalable_Sequential_3D_Reconstruction_wi",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.CV_2508.10893v1_STream3R-Scalable-Sequential-3D-Reconstruction-wi with content analysis. Detected project type: computer vision (confidence score: 8 matches).",
    "key_algorithms": [
      "Decoder-Only",
      "Geometric",
      "Data-Driven",
      "Language",
      "Scene",
      "Pointmap",
      "Frame",
      "Online",
      "Trained",
      "Streaming"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.CV_2508.10893v1_STream3R-Scalable-Sequential-3D-Reconstruction-wi.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nSTREAM 3R: Scalable Sequential 3D Reconstruction\nwith Causal Transformer\nYushi Lan1\u2217, Yihang Luo1\u2217, Fangzhou Hong1, Shangchen Zhou1,\nHonghua Chen1,Zhaoyang Lyu2,Shuai Yang3,Bo Dai4,Chen Change Loy1,Xingang Pan1\n1S-Lab, Nanyang Technological University, Singapore\n2Shanghai Artificial Intelligence Laboratory3WICT, Peking University4The University of Hong Kong\nhttps://nirvanalan.github.io/projects/stream3r\nCache\tFeatures \tof\tView\t1 Cache\tFeatures \tof\tView\t1,\t2 Cache\tFeatures \tof\tView\t1,\t2,\t3\nFigure 1: STREAM 3R. Given a stream of input images, our method estimates dense 3D geometry\nfor each incoming frame using a causal Transformer. Features from previously observed frames are\ncached as context for future inference.\nAbstract\nWe present STREAM 3R, a novel approach to 3D reconstruction that reformulates\npointmap prediction as a decoder-only Transformer problem. Existing state-of-\nthe-art methods for multi-view reconstruction either depend on expensive global\noptimization or rely on simplistic memory mechanisms that scale poorly with\nsequence length. In contrast, STREAM 3Rintroduces an streaming framework that\nprocesses image sequences efficiently using causal attention, inspired by advances\nin modern language modeling. By learning geometric priors from large-scale 3D\ndatasets, STREAM 3Rgeneralizes well to diverse and challenging scenarios, includ-\ning dynamic scenes where traditional methods often fail. Extensive experiments\nshow that our method consistently outperforms prior work across both static and\ndynamic scene benchmarks. Moreover, STREAM 3Ris inherently compatible with\nLLM-style training infrastructure, enabling efficient large-scale pretraining and\nfine-tuning for various downstream 3D tasks. Our results underscore the potential\nof causal Transformer models for online 3D perception, paving the way for real-\ntime 3D understanding in streaming environments. More details can be found in\nour project page.\n*Equal contribution.\nPreprint. Under review.arXiv:2508.10893v1  [cs.CV]  14 Aug 2025\n\n--- Page 2 ---\n1 Introduction\nReconstructing detailed 3D geometry from images is the crux in computer vision [ 1,2,3] and serves\nas the pre-requisite for a series of downstream applications, like autonomous driving [ 4], virtual\nreality [ 5,6], robotics [ 7], and more. While traditional visual-geometry methods like SfM [ 1] and\nMulti-view Stereo [ 8,9] tackle this problem by solving a series of sub-problems through handcrafted\ndesigns, a recent trend led by DUSt3R [ 10] has demonstrated a promising new way of directly\nregressing point clouds using powerful transformers. This paradigm, along with its follow-up works\nincluding MASt3R [ 11], Fast3R [ 12], and VGG-T [ 13], enables the reconstruction of 3D geometry\nfrom a number of input images\u2013ranging from a single image to hundreds\u2013offering a more unified\nsolution to 3D reconstruction.\nWhile these works focus on processing a fixed set of images, real-world applications often require\ncontinuously processing streaming visual input and updating the reconstruction on-the-fly [ 14], such\nas when an autonomous agent explores a new environment or when processing a long video sequence.\nHandling streaming input poses significant new challenges. For example, naively running Fast3R or\nVGG-T every time a new image arrives would incur significant redundant computation, as they have\nto reconstruct from scratch without inheriting previous results. These methods also struggle with long\nvideos due to the expensive full-attention operation. Spann3R [ 15] extends DUSt3R with a memory\ndesign [ 16] to support incremental reconstruction, but it still suffers from significant accumulated\ndrift and fails over dynamic scenes. The most relevant concurrent work is CUT3R [ 17], which\nproposes a RNN paradigm [ 18] to handle unstructured or streaming inputs. However, the RNN-based\ndesign does not scale well with modern network architectures [ 19] and struggles with long-range\ndependency due to its limited memory size.\nIn light of the streaming nature of this task, in this work, we are interested in investigating the use of\na transformer with uni-directional causal attention to achieve online, incremental 3D reconstruction.\nIn an LLM-style transformer with causal attention, the prediction at each step reuses previous\ncomputations through a KVCache, which is proved successful in many language and audio tasks [ 20,\n21]. We observe that this property is also highly desirable for addressing online 3D reconstruciton\nfrom streaming data, as each step should build upon the previous reconstruction while integrating\nnew content from the incoming frame.\nMotivated by this, we propose STREAM 3R, a comprehensive framework that performs 3D recon-\nstruction from unstructured or streaming input images, and predicts the corresponding point maps in\nboth world and local coordinates [ 12]. Unlike concurrent works [ 12,13] that resolve this issue by\nreplacing DUSt3R\u2019s asymmetric decoders with bi-directional attention blocks [ 22,23],STREAM 3R\nfollows the modern decoder-only [24] transformer design, where incoming frames are sequentially\nprocessed and registered with causal attention [ 25]. In this way, STREAM 3Ris naturally compatible\nwith modern Large Language Models (LLMs) [ 20] training and inference techniques such as window\nattention [ 26] and KVCache [ 24], i.e., the tokens of processed observations will be saved as reference\nfor registering incoming frames.\nWe train our method end-to-end on a large collection of 3D data, and benchmark the proposed method\non a series of downstream applications. In summary, our key contributions are as follows:\n1.We propose STREAM 3R, a decoder-only transformer framework that reformulates dense 3D\nreconstruction into a sequential registration task with causal attention, enabling scalability\nto unstructured and streaming inputs.\n2.STREAM 3Ris inherently compatible with modern LLM-style training and inference tech-\nniques, allowing efficient and scalable context accumulation across frames.\n3.Our architecture supports both world- and local-coordinate pointmap prediction, and natu-\nrally generalizes to large-scale novel view synthesis scenarios via splatting-based rendering.\n4.We train the model end-to-end on diverse 3D data and demonstrate competitive or superior\nperformance on standard benchmarks, with strong generalization and fast inference speed.\n2 Related Work\nClassic 3D Reconstruction. Early 3D reconstruction pipelines \u2013 such as Structure-from-Motion\n(SfM) [ 27,1,28] and SLAM [ 14,29,30] \u2013 estimate sparse geometry and camera poses from\n2\n\n--- Page 3 ---\nimage collections via geometric reasoning. More recent approaches such as NeRF [ 31,32,33] and\nGaussian Splatting [ 34,35] shift the focus to high-fidelity novel view synthesis using continuous\nvolumetric representations. However, these methods are typically trained per-scene with no learned\npriors, leading to slow convergence and poor generalization to sparse or occluded inputs\u2014a limitation\nsometimes referred to as the tabula rasa assumption [ 17]. In contrast, we adopt a data-driven approach\nthat learns geometric priors from large-scale 3D datasets [ 36,37], enabling fast and generalizable\nreconstruction from unstructured or streaming inputs.\nLearning 3D Priors from Data. Recent works leverage large-scale data to learn priors for depth\nestimation [ 38,39,40], pose+depth estimation [ 41,42], and bundle adjustment [ 43]. While these\nmethods improve generalization, most focus on monocular depth or two-view setups, limiting their\nability to reconstruct full geometry in the absence of known intrinsics [ 44]. VGGSfM [ 43] introduces\ndifferentiable bundle adjustment by integrating neural feature matching with classic optimization, but\nremains iterative and computationally heavy, impeding scalability. In the multi-view stereo domain,\napproaches such as MVSNeRF [ 3,45] and MVSNet [ 8] integrate neural networks into the MVS\npipeline but typically require known camera poses and still heavily rely on hand-crafted components\nto effectively incorporate 3D geometry.\nPointmap-based Representations. Pointmap-based representations [ 10,11,46,47,48,49,50] have\nrecently emerged as a unifying format for dense 3D geometry prediction, aligning well with the\noutput structure of neural networks. Compared to voxels [ 51], meshes [ 52], or implicit fields [ 53,31],\npointmaps enable feedforward inference and real-time rendering, and can directly support applications\nsuch as rasterization-based rendering [ 34], SLAM [ 54,55], and few-shot synthesis [ 56]. DUSt3R [ 10]\nand follow-ups like MASt3R [ 11] recast stereo 3D reconstruction as dense pointmap regression,\njointly estimating depth, pose, and intrinsics from image pairs. However, their pairwise design\nfundamentally limits scalability \u2013 requiring quadratic fusion operations and complex global alignment\nprocedures when handling multi-view scenarios. Our approach maintains the advantages of pointmap\nrepresentations while overcoming these scalability limitations.\n4D Reconstruction from Monocular Videos. Reconstructing dense geometry of dynamic scenes\nfrom monocular video is significant but challenging for conventional methods. Recent methods [ 57,\n58,41,59] leverages depth priors to resolve this challenge. Specifically, Robust-CVD [ 59] and\nMegaSAM [ 41] requires time-consuming per-video optimization. MonST3R [ 49] builds on DUSt3R\nto output pointmaps for dynamic scenes by fine-tuning DUSt3R on the dynamic datasets. However, it\nstill requires a sliding-window based per-video global alignment as post-processing. In contrast, our\nmethod enables feedforward 4D reconstruction directly from monocular videos, supporting online\nprediction without costly per-video optimization or post-processing alignment.\nReconstruction Methods from Streaming Inputs. Streaming approaches offer a more scal-\nable alternative solution for the 3D reconstruction problem, represented by the monocular SLAM\npipelines [ 14,55,60]. Inspired by the existing learning-based online 3D reconstruction meth-\nods [ 61,62,63], recently Spann3R [ 15] introduces a memory-based extension to DUSt3R, while\nFast3R [ 12] and VGG-T [ 13] replace asymmetric decoders with Transformer-based attention stacks\nto directly enable multi-view fusion. Despite these advances, these approaches still predominantly\nrely on global full-attention mechanisms, limiting their real-time scalability with increasing sequence\nlength. CUT3R [ 17] adopts an RNN-style architecture to process unstructured inputs incrementally,\nbut suffers from limited memory capacity and poor compatibility with modern hardware acceleration\ntechniques [ 19]. Our method fundamentally reconceptualizes pointmap prediction as a decoder-only\nTransformer task, enabling efficient causal inference through techniques like KVCache and win-\ndowed attention [ 26,24]. This architectural design allows us to scale effectively to long sequences\nwhile maintaining full compatibility with modern LLM-style training infrastructure and optimization\ntechniques, overcoming the limitations of previous approaches.\n3 Preliminaries: DUSt3R\nWe reformulate DUSt3R [ 10] to accept a stream of images as input. In DUSt3R, each incoming\nimage Itis initially patchified into a set of Ktokens, Ft= Encoder( It), where Ft\u2208RK\u00d7Cand\nEncoder is a weight-sharing ViT [ 64]. Specifically, DUSt3R is designed to ingest two input images\nat a time, i.e., t\u2208 {1,2}. The encoded images yield two sets of tokens:\nF1= Encoder( I1),F2= Encoder( I2). (1)\n3\n\n--- Page 4 ---\nDecoder\nSelf\tAttnCausal\tAttn\n.\t.\t.Image\t1\nImage\t2\nImage\t3ViTEncoder\n.\t.\t.StreamingInput\tImages\n\ud83d\udd25Reg\tToken\nIncremental\t3D\tReconstructionsXYZ\tand\tConfidence\t\u2a01Self\tAttn\nDecoderSelf\tAttnCausal\tAttn\nViTEncoderViTEncoderGlobalLocal\nGlobalLocal\nGlobalLocal\nTwo\tViews\nMore\tViewsMemory\tCache\nMemory\tCacheMemory\tCache\tInitCamera\tToken\n\ud83d\udd25\nCamera\tToken\n\ud83d\udd25\u2a01\nCameraToken\n\ud83d\udd25\u2a01Figure 2: Method Overview. Built on a causal transformer, STREAM 3Rprocesses streaming images\nsequentially for 3D reconstruction. Each input image is first tokenized using a shared-weight ViT\nencoder, and the resulting tokens are passed to our causal decoder. Each decoder layer begins with\nframe-wise self-attention. For subsequent views, the model applies causal attention to the memory\ntokens cached from previous observations. The outputs include point maps and confidence maps in\nboth world and camera coordinate systems, as long as the camera pose as shown on the right. Note\nthat we visualize the point cloud of the Head localwith its depth map.\nAfterwards, the decoder networks Decoder treason over both of them through a series of transformer\nblocks with cross attention layer:\nGi\n1= DecoderBlocki\n1(Gi\u22121\n1, Gi\u22121\n2), Gi\n2= DecoderBlocki\n2(Gi\u22121\n2, Gi\u22121\n1), (2)\nwithiranging from 1toB, representing the block index in a decoder of Bblocks in total. G0\n1:=F1\nandG0\n2:=F2. Finally, the corresponding regression head of each branch predicts a pointmap with\nan associated confidence map:\n\u02c6X1,1,\u02c6C1,1= Head 1(G0\n1, . . . , GB\n1),\u02c6X2,1,\u02c6C2,1= Head 2(G0\n2, . . . , GB\n2). (3)\nNote that DUSt3R is designed for two-view inputs and requires an expensive and unscalable global\nalignment process to incorporate more input views.\n4 Method\nWe introduce STREAM 3R, a transformer that ingests uncalibrated streaming images as inputs and\nyields a series of 3D attributes as output. The input can be either unstructured image collections or\nvideo. Unlike existing approaches [ 13,12] that address this issue by adopting costly bi-directional\nattention over the entire input sequence or using fixed-size memory buffers [ 15,17],STREAM 3R\ninstead caches the features from the past frames as context and sequentially processes the incoming\nframe by performing causal attention over the accumulated observations. This design not only enables\nfaster training and quicker convergence but also aligns with the architectural principles of modern\nLLMs, allowing us to leverage the advancement of that domain. We first introduce the problem\nformulation in Sec. 4.1, the architecture in Sec. 4.2, and the training objectives in Sec. 4.3, and the\nimplementation details in Sec. 5. An overview of the proposed method is shown in Fig. 2.\n4.1 Problem Definition and Notation\nSTREAM 3Ris a regression model that sequentially takes a streaming of NRGB images (I)N\nt, where\neach image I\u2208R3\u00d7H\u00d7Wbelongs to the same 3D scene. The streaming inputs are successively\ntransformed into a set of 3D annotations corresponding to each frame:\nf\u03b8((I)N\nt) = ( \u02c6Xlocal\nt,\u02c6Xglobal\nt,\u02c6Pt)N\nt. (4)\n4\n\n--- Page 5 ---\nTechnically, STREAM 3Ris implemented as a causal transformer that maps each image Itinto its\ncorresponding pointmap of the local coordinate \u02c6Xlocal\nt\u2208R3\u00d7H\u00d7Wand its pointmap in a global\ncoordinate \u02c6Xglobal\nt\u2208R3\u00d7H\u00d7W, which is indicated by the first input frame I0, and its relative camera\npose \u02c6Pt\u2208R9including both intrinsics and extrinsics. We devise later how these 3D attributes are\npredicted.\n4.2 Causal Transformer for 3D Regression\nCausal Attention for Long-context 3D Reasoning. As mentioned in Sec. 3, given the streaming\ninputs, for each current image, It, our method first tokenizes it into the features Ft= Encoder( It).\nThe main difference lies in the decoder side: rather than performing bi-directional attention over\nthe whole sequence [ 12] or interacting with a learnable state as in RNN [ 17], we draw inspiration\nfrom the LLMs [ 20,24,65] and perform causal attention efficiently with previous observations.\nSpecifically, after performing frame-wise self-attention in each decoder block, the current feature\nGi\u22121\ntwill cross-attend to the features of previously observed frames corresponding to the same layer:\nGi\nt= DecoderBlocki\u0000\nGi\u22121\nt, Gi\u22121\n0\u2295Gi\u22121\n1\u2295 \u00b7\u00b7\u00b7 \u2295 Gi\u22121\nt\u22121\u0001\n. (5)\nThis interaction ensures efficient information transfer to handle long-context dependencies. Note that\nthis operation is easy to implement and well optimized with KV cache during inference for efficient\ncomputation [24, 20].\nSimplified Decoder Design. To achieve this, several network architecture modifications are required.\nIn DUSt3R, the decoder follows a symmetric design, i.e., two separate decoders Decoder 1,Decoder 2\nare employed to handle two input views. To extend to an arbitrary number of inputs, we remove\nthe symmetric design and only retain a single decoder Decoder to process all the input frames.\nSpecifically, each block in the decoder contains a SelfAttn block for frame-wise attention, and\naCrossAttn block for causally attending to the features of all previous observations. Note that\nwe process the first two frames following the convention of DUSt3R due to the lack of historical\ncontext. All incoming frames afterwards follow the causal operation in Eq. (5). Note that to indicate\nthe canonical world space, we add a learnable register token [reg] to the tokens of the first frame\nF1=F1+ [reg] , in an element-wise manner, as shown in Fig. 2. In this way, the model learns to\noutput the global points without introducing Nseparate decoders. Unlike existing work [ 12], we did\nnot impose positional embedding for other frames for simplicity.\nPrediction Heads. After the decoding operation, the 3D attributes corresponding to each frame can\nbe predicted accordingly. Following existing works [ 12,17,13], we predict two sets of point maps\n\u02c6Xlocal\nt,\u02c6Xglobal\nt with their corresponding confidence maps \u02c6Clocal\nt,\u02c6Cglobal\nt. Specifically, the local point\nmap \u02c6Xlocal\ntis defined in the coordinate frame of the viewing camera, and the global point map \u02c6Xglobal\nt\nis in the coordinate frame of the first image I1. We use two DPT [ 66] heads for point map prediction:\n\u02c6Xlocal\nt,\u02c6Clocal\nt= Head local(G0\nt, . . . , GB\nt), (6)\n\u02c6Xglobal\nt,\u02c6Cglobal\nt = Head global(G0\nt, . . . , GB\nt), (7)\n\u02c6Pt= Head pose(G0\nt, . . . , GB\nt), (8)\nwhere this redundant prediction has been demonstrated to simplify training [ 67,12] and facilitates\ntraining on 3D datasets with partial annotations, e.g., single-view depth datasets [68].\n4.3 Training Objective\nSTREAM 3Ris trained using a generalized form of the pointmap loss introduced in DUSt3R. Given\na sequence of Nrandomly sampled images, sourced either from a video or an image collection,\nwe train the model to produce pointmap predictions denoted by X={\u02c6Xlocal,\u02c6Xglobal}, where\n\u02c6Xlocal={\u02c6Xlocal\nt}N\nt=1and\u02c6Xglobal={\u02c6Xglobal\nt}N\nt=1. The corresponding confidence scores are denoted\nas\u02c6C.\nFollowing DUSt3R [10], we apply a confidence-aware regression loss to the pointmaps:\nLconf=X\n(\u02c6x,\u02c6c)\u2208(\u02c6X,\u02c6C)\u0012\n\u02c6c\u00b7\n\n\n\n\u02c6x\n\u02c6s\u2212x\ns\n\n\n\n2\u2212\u03b1log \u02c6c\u0013\n, (9)\n5\n\n--- Page 6 ---\nInput Images MonST3R Fast3R Ours CUT3RFigure 3: Qualitative results on in-the-wild Images. We compare our method with approaches\nMonST3R, Fast3R, and CUT3R, and demonstrate that it achieves superior visual quality.\nwhere \u02c6sandsare scale normalization factors for \u02c6XandXfor scale-invariant supervision [ 69].\nWe also set \u02c6s:=sfor metric-scale datasets as in MASt3R [ 11] to enable metric-scale pointmaps\nprediction. For the camera prediction loss, we parameterize pose \u02c6Ptas quaternion \u02c6qt, translation \u02c6\u03c4t\nand focal \u02c6ft, and minimize the L2 norm between the prediction and ground truth:\nLpose=NX\nt=1\u0012\n\u2225\u02c6qt\u2212qt\u22252+\n\n\n\n\u02c6\u03c4t\n\u02c6s\u2212\u03c4t\ns\n\n\n\n2+\n\n\n\u02c6ft\u2212ft\n\n\n2\u0013\n. (10)\n5 Experiments\nDatasets. We train our method on a large and diverse collection of 3D datasets, e.g., Co3Dv2 [ 37],\nScanNet++ [ 70], ScanNet [ 71], HyperSim [ 72], Dynamic Replica [ 73], DL3DV [ 36], Blended-\nMVS [ 74], Aria Synthetic Environments [ 75], TartanAir [ 76], MapFree [ 77], MegaDepth [ 78], and\nARKitScenes [79]. Please check the supplement for the full dataset details.\nImplementation Details. We provide two version of STREAM 3R, where STREAM 3R\u03b1is inspired\nand fine-tuned from DUSt3R [ 10] pre-trained weights, and STREAM 3R\u03b2is initialized from the\nflagship VGG-T [ 13] model. For STREAM 3R\u03b1, we inherit the 24-layer CroCo ViT [ 80,81] as our\nencoder, and retrofit its 12-layer decoder network by only retaining the first decoder Decoder =\nDecoder 1. The DPT-L [ 66] heads are used to map the decoded tokens to the local and global point\nmaps accordingly. For STREAM 3R\u03b2, we replace the SelfAttn layer in the Global Attention of\nVGG-T with CausalAttn and fine-tune all the parameters. For memory-efficient and stable training,\nwe inject QK-Norm [ 82] to each transformer layer and leverage FlashAttention [ 19] for BFloat16\nmixed precision training.\nTraining Details. Our model is trained with the AdamW optimizer on a batch size of 64with a\nlearning rate 1e-4 for 400Kiterations. For each batch, we randomly sample 4\u221210frames from a\nrandom training scene. The input frames are cropped into diverse resolutions, ranging from 224\u00d7224\nto512\u00d7384to improve generality. The training runs end-to-end on 8NVIDIA A100 GPUs over\nseven days. Gradient checkpointing is also adopted to optimize memory usage.\nBaselines. We compare our methods against a set of baselines that are designed to take a pair of views\nas input: DUSt3R [ 10], MASt3R [ 11], and MonST3R [ 49]. Besides, we include the comparison\nagainst concurrent methods Spann3R [ 15], CUT3R [ 17], SLAM3R [ 55], and Fast3R [ 12] that are\n6\n\n--- Page 7 ---\nTable 1: Single-frame Depth Evaluation. We report the performance on Sintel, Bonn, KITTI, and\nNYU-v2 (static) datasets. The best and second best results in each category are bold andunderlined\nrespectively. Our method achieves better or comparable performance against existing methods.\nMethodSintel Bonn KITTI NYU-v2\nAbs Rel \u2193\u03b4<1.25\u2191Abs Rel \u2193\u03b4<1.25\u2191Abs Rel \u2193\u03b4<1.25\u2191Abs Rel \u2193\u03b4<1.25\u2191\nVGG-T 0.271 67.7 0.053 97.3 0.076 93.3 0.060 94.8\nFast3R 0.502 52.8 0.192 77.3 0.129 81.2 0.099 88.9\nDUSt3R 0.424 58.7 0.141 82.5 0.112 86.3 0.080 90.7\nMASt3R 0.340 60.4 0.142 82.0 0.079 94.7 0.129 84.9\nMonST3R 0.358 54.8 0.076 93.9 0.100 89.3 0.102 88.0\nSpann3R 0.470 53.9 0.118 85.9 0.128 84.6 0.122 84.9\nCUT3R 0.428 55.4 0.063 96.2 0.092 91.3 0.086 90.9\nSTREAM 3R\u03b10.350 59.0 0.075 93.4 0.088 91.3 0.091 89.9\nSTREAM 3R\u03b20.228 70.7 0.061 96.7 0.063 95.5 0.057 95.7\nspecifically designed for handling a varying number of input images. We also include the flagship 3D\ngeometry model VGG-T [ 13] for reference. Note that Fast3R and VGG-T are bi-directional attention\nmethods, and we group them together with methods that require global optimization (GA). We group\nother concurrent methods together as streaming methods that supports processing sequential inputs.\nNote that for all methods except for VGG-T and STREAM 3R\u03b2, we conduct inference with the largest\ndimension of 512. For VGG-T based methods, we conduct inference with the largest dimension of\n518due to the requirement of DINO-V2 tokenizer [ 83]. Regarding FPS, we benchmark the inference\nspeed on the A100 GPU with FP32. Comparisons of more concurrent methods [ 84,38] are included\nin the supplement.\n5.1 Monocular and Video Depth Estimation\nMono-Depth Estimation. Following previous methods [ 49,17], we first evaluate monocular depth\nestimation on Sintel [ 85], Bonn [ 86], KITTI [ 4], and NYU-v2 [ 87] datasets, which cover dynamic and\nstatic, indoor and outdoor, realistic and synthetic data. These datasets are not used for training and\nare suitable for benchmarking the zero-shot performance across different domains. Our evaluation\nincludes the absolute relative error (Abs Rel) and percentage of inlier points within a 1.25-factor of\ntrue depth \u03b4 <1.25, following the convention of existing methods [ 40,88]. Per-frame median scaling\nis imposed as in DUSt3R. We include the quantitative results in Tab. 1. As can be seen, our method\nachieves state-of-the-art compared to streaming-based methods, and even performs best compared to\nVGG-T on Sintel, KITTI, and NYU-2. Also note that our method uses fewer datasets and compute\nresources compared to CUT3R. Specifically, CUT3R adopts a curriculum training of four stages for\n100 + 35 + 40 + 10 = 185 epochs, while our method is trained end-to-end for only 7epochs using a\npartial of CUT3R\u2019s datasets due to the computational resources constraints.\nVideo Depth Estimation. We also benchmark our model on the video depth task, which evaluates\nboth per-frame depth quality and inter-frame depth consistency by aligning the output depth maps\nto the ground truth depth maps using a given per-sequence scale. Metric point map methods\nlike MASt3R, CUT3R, and ours are also reported without alignment. The quantitative results for\nboth methods are included in Tab. 5. Over per-sequence scale alignment, our method surpasses\noptimization-based baselines DUSt3R-GA [ 10] and MASt3R-GA [ 11] (static-scene assumption) and\neven MonST3R-GA [ 49] (dynamic-scene, optical flow [ 89] dependent). Against the streaming state-\nof-the-art CUT3R, we achieve higher accuracy on all three benchmarks while running 40% faster.\nSTREAM 3Ralso outperforms full-attention Fast3R [ 12], streaming approaches Spann3R [ 15], and\nthe flagship model VGG-T on Sintel. Notably, STREAM 3R\u03b2-W, using sliding-window attention [ 26]\nfor constant cache, exceeds STREAM 3R\u03b2on Bonn and KITTI despite accessing only five past frames.\n5.2 3D Reconstruction\nWe also benchmark scene-level 3D reconstruction on the 7-scenes [ 90] dataset and use accuracy (Acc),\ncompletion (Comp), and normal consistency (NC) metrics, following the convention of existing\nmethods [ 15,17,10]. Following CUT3R, we assess the model\u2019s performance on image collections\nwith minimal or no overlap by evaluating using sparsely sampled images, i.e., 3to5frames per scene.\nThe quantitative results are included in Tab. 3. Our method achieves better performance compared to\n7\n\n--- Page 8 ---\nTable 2: Video Depth Evaluation. We evaluate scale-invariant and metric depth accuracy on the\nSintel, Bonn, and KITTI datasets. Methods that require global alignment are denoted as \"GA\". The\n\"Type\" column indicates whether the method is Optimzation-based (\"Optim), streaming (\"Stream\"),\nor full-attention (\"FA) We also report inference speed in FPS on the KITTI dataset using 512 \u00d7144\nresolution for all methods on an A100 GPU, except for Spann3R, which supports Stream 224 \u00d7224\ninputs. Our method achieves performance that is better than CUT3R, while offering FAter inference.\nForSTREAM 3R\u03b2-W[5], we indicate using sliding window attention on STREAM 3R\u03b2with window\nsize 5. Note that STREAM 3R\u03b2-W[5] achieves the fastest FPS among all streaming-based methods.\nSintel Bonn KITTI\nAlignment Method Type Abs Rel \u2193\u03b4<1.25\u2191Abs Rel \u2193\u03b4<1.25\u2191Abs Rel \u2193\u03b4<1.25\u2191FPS\nVGG-T [13] FA 0.297 68.8 0.055 97.1 0.073 96.5 7.32\nFast3R [12] FA 0.653 44.9 0.193 77.5 0.140 83.4 47.23\nPer-sequence scaleDUSt3R-GA [10] Optim 0.656 45.2 0.155 83.3 0.144 81.3 0.76\nMASt3R-GA [11] Optim 0.641 43.9 0.252 70.1 0.183 74.5 0.31\nMonST3R-GA [49] Optim 0.378 55.8 0.067 96.3 0.168 74.4 0.35\nSpann3R [15] Stream 0.622 42.6 0.144 81.3 0.198 73.7 13.55\nCUT3R [17] Stream 0.421 47.9 0.078 93.7 0.118 88.1 16.58\nSTREAM 3R\u03b1Stream 0.478 51.1 0.075 94.1 0.116 89.6 23.48\nSTREAM 3R\u03b2Stream 0.264 70.5 0.069 95.2 0.080 94.7 12.95\nSTREAM 3R\u03b2-W[5] Stream 0.279 68.6 0.064 96.7 0.083 95.2 32.93\nMetric scaleMASt3R-GA [11] Optim 1.022 14.3 0.272 70.6 0.467 15.2 0.31\nCUT3R Stream 1.029 23.8 0.103 88.5 0.122 85.5 16.58\nSTREAM 3R\u03b1Stream 1.041 21.0 0.084 94.4 0.234 57.6 23.48\nTable 3: 3D Reconstruction Evaluation on 7-Scenes [ 90].Despite operating in the streaming\nsetting, our method delivers competitive performance, matching or even exceeding that of offline\noptimization-based methods that leverage global alignment. STREAM 3R\u03b2-FA indicates adopting full\nattention in our trained model for 3D reconstruction.\nMethod TypeAcc\u2193 Comp\u2193 NC\u2191\nFPSMean Med. Mean Med. Mean Med.\nVGG-T [13] FA 0.087 0.039 0.091 0.039 0.787 0.890 12.0\nFast3R [12] FA 0.164 0.108 0.163 0.080 0.686 0.775 30.92\nDUSt3R-GA [10] Optim 0.146 0.077 0.181 0.067 0.736 0.839 0.68\nMASt3R-GA [11] Optim 0.185 0.081 0.180 0.069 0.701 0.792 0.34\nMonST3R-GA [49] Optim 0.248 0.185 0.266 0.167 0.672 0.759 0.39\nSTREAM 3R\u03b2-FA Stream 0.091 0.043 0.075 0.042 0.769 0.879 12.0\nSpann3R [15] Stream 0.298 0.226 0.205 0.112 0.650 0.730 12.97\nSLAM3R [55] Stream 0.287 0.155 0.226 0.066 0.644 0.720 38.40\nCUT3R [17] Stream 0.126 0.047 0.154 0.031 0.727 0.834 17.00\nSTREAM 3R\u03b1Stream 0.148 0.077 0.177 0.058 0.700 0.801 26.4\nSTREAM 3R\u03b2Stream 0.122 0.044 0.110 0.038 0.746 0.856 20.12\noptimization-based methods and strong baselines including Spann3R, Fast3R, CUT3R, and SLAM3R.\nCompared to CUT3R, our method shows better performance with over 50% times faster during the\ninference. While SLAM3R achieves the fastest inference, it yields noticeably lower reconstruction\naccuracy than our method. This performance gap can be partially attributed to SLAM3R being\ntrained and evaluated at a lower input resolution of 224\u00d7224. We also include STREAM 3R\u03b2-FA for\ncomparison, which indicates replacing the causal attention in STREAM 3R\u03b2into full attention (FA).\nInterestingly, STREAM 3R\u03b2-FA yields comparable performance compared to VGG-T and even better\nresults on the completion metric. This highlights the effectiveness and generality of our proposed\nmethod. The comparison results on NRGBD [91] benchmark is included in the supplement.\n5.3 Camera Pose Estimation\nFollowing CUT3R [ 17], we evaluate camera pose estimation accuracy on the Sintel [ 85], TUM-\ndynamics [ 92], and ScanNet [ 71] datasets. Sintel and TUM-dynamics both feature substantial\ndynamic motion, posing significant challenges to conventional SfM and SLAM pipelines. We report\nAbsolute Translation Error (ATE), Relative Translation Error (RPE trans), and Relative Rotation Error\n8\n\n--- Page 9 ---\nTable 4: Camera Pose Evaluation on Sintel [ 85], TUM-dynamic [ 92], and ScanNet [ 71] datasets.\nOur method achieves comparable performance with CUT3R on most benchmarks.\nSintel TUM-dynamics ScanNet\nMethod Type ATE\u2193RPE trans \u2193RPE rot \u2193ATE\u2193RPE trans \u2193RPE rot \u2193ATE\u2193RPE trans \u2193RPE rot \u2193\nParticle-SfM [93] Optim 0.129 0.031 0.535 - - - 0.136 0.023 0.836\nRobust-CVD [59] Optim 0.360 0.154 3.443 0.153 0.026 3.528 0.227 0.064 7.374\nCasualSAM [94] Optim 0.141 0.035 0.615 0.071 0.010 1.712 0.158 0.034 1.618\nDUSt3R-GA [10] Optim 0.417 0.250 5.796 0.083 0.017 3.567 0.081 0.028 0.784\nMASt3R-GA [11] Optim 0.185 0.060 1.496 0.038 0.012 0.448 0.078 0.020 0.475\nMonST3R-GA [49] Optim 0.111 0.044 0.869 0.098 0.019 0.935 0.077 0.018 0.529\nDUSt3R [10] Onl 0.290 0.132 7.869 0.140 0.106 3.286 0.246 0.108 8.210\nSpann3R [15] Onl 0.329 0.110 4.471 0.056 0.021 0.591 0.096 0.023 0.661\nCUT3R [17] Onl 0.213 0.066 0.621 0.046 0.015 0.473 0.099 0.022 0.600\nSTREAM 3R\u03b2Onl 0.213 0.076 0.868 0.026 0.013 0.330 0.052 0.021 0.850\n(RPE rot) after Sim(3) alignment with the ground truth, following the protocol in [ 30,49,17]. Our\napproach operates without requiring camera calibration, similar to the compared baselines [ 30]. While\nmany prior methods [ 59,94] address this via test-time optimization\u2014jointly estimating intrinsics and\ndense depth for each sequence\u2014we focus on purely online processing. Table 4 reports results for\nOnline (Onl) and Optimization (Optim) categories, with DUSt3R [ 10] included in the latter (aligning\nall frames to the first frame without global alignment). Although optimization-based systems still\nachieve the lowest errors overall, our method establishes the strongest performance among streaming\napproaches, and notably surpasses CUT3R [ 17] on both TUM-dynamics and ScanNet, demonstrating\nparticular robustness in dynamic environments.\n5.4 Ablation on the Effectiveness of the Proposed Architecture\nHere, we conduct detailed ablation analysis on STREAM 3Rto demonstrate the effectiveness of its\ndesigns. Due to the extensive computational resources required to train the model, we only train the\nablation models on 224\u00d7224resolution images. All the datasets are included to train the models.\nNote that for a fair comparison, we initialize all the models below using the pre-trained MASt3R [ 11]\ncheckpoints and train the models using the same hyper-parameters and compute resources.\n0 5000 10000 15000 20000\nTraining Step123Total LossOurs\nCUT3R\n(a) Overall Training Curve\n0 5000 10000 15000 20000\nTraining Step0.20.40.60.8Local Head LossOurs\nCUT3R (b) Training Curve of Local Branch\n0 5000 10000 15000 20000\nTraining Step0.30.40.50.60.7Global Head LossOurs\nCUT3R (c) Training Curve of Global Branch\nFigure 4: Ablation of our proposed STREAM 3R. Compared to RNN-based architecture [ 17], our\ndecoder-only network yields better convergence with faster training speed in the 3D point map\nprediction task, especially in the global branch.\nWe demonstrate the effectiveness of decoder-only transformer against RNN design in the sequential\n3D pointmap prediction. The main baseline is CUT3R [ 17], which leverages the RNN design to\nachieve this. For a fair comparison, we re-train CUT3R and our method using the same dataset and\npre-trained model weights initialization. We include the training curve in Fig. 4a, where both models\nare trained with the same hyperparameters and compute resources. As can be observed, STREAM 3R\nconverges faster compared to CUT3R and performs 60% more training steps within the given time.\nThis may sound counterintuitive since STREAM 3Ris attending to a longer context against CUT3R\u2019s\nconstant state memory. However, since CUT3R architecture requires a state-update operation after\neach state-readout interaction, while STREAM 3Rdirectly attends to cached features of existing\nobservations.\n9\n\n--- Page 10 ---\nTable 5: Ablation on Video Depth Estimation . When evaluating the checkpoint trained for the\nsame number of iterations, our proposed architecture consistently achieves better performance against\nRNN-based CUT3R on the video depth estimation task.\nMethodSintel BONN KITTI\nAbs Rel \u2193 \u03b4<1.25\u2191 Abs Rel \u2193 \u03b4<1.25\u2191 Abs Rel \u2193 \u03b4<1.25\u2191\nCUT3R 0.598 40.7 0.102 90.7 0.157 77.4\nSTREAM 3R\u03b10.535 47.0 0.083 94.2 0.141 81.8\nTable 6: Ablation on 3D Reconstruction on 7-Scenes . Our proposed architecture consistently\nachieves better performance against RNN-based CUT3R on the 3D reconstruction task when trained\nunder the same configurations. Note that our architecture is trained even faster.\nMethodAcc\u2193 Comp\u2193 NC\u2191\nMean Med. Mean Med. Mean Med.\nCUT3R 0.480 0.365 0.330 0.148 0.555 0.583\nSTREAM 3R\u03b10.328 0.261 0.255 0.095 0.605 0.659\nWe also notice in Fig. 4b that the convergence of Head localis similar among the two architectures,\nwhile for Head global, our proposed architecture shows noticeably faster convergence speed, as shown\nin Fig. 4c. This demonstrates that using a single state makes the model harder to register incoming\nframes due to the limited memory capacity.\nQuantitatively, we benchmark the ablation models on both the video depth estimation in Tab. 5 and\n3D reconstruction in Tab. 6, which evaluates the Head localandHead global correspondingly. For a\nfair comparison, we evaluate the checkpoints trained for the same number of iterations. As can be\nobserved, our proposed architecture consistently achieves better performance on both tasks.\n6 Conclusion and Discussions\nWe have introduced STREAM 3R, a decoder-only transformer framework for dense 3D reconstruction\nfrom unstructured or streaming image inputs. By reformulating reconstruction as a sequential\nregistration task with causal attention, STREAM 3Rovercomes the scalability bottlenecks of prior\nwork and aligns naturally with LLM-style training and inference pipelines. Our design allows\nefficient integration of geometric context across frames, supports dual-coordinate pointmap prediction,\nand generalizes to novel-view synthesis over large-scale scenes without the need for global post-\nprocessing. Through extensive experiments across standard benchmarks, we show that STREAM 3R\nachieves competitive or superior performance in the monocular/video-depth estimation and 3D\nreconstruction tasks, with significantly improved inference efficiency. By bridging geometric learning\nwith scalable sequence modeling, we hope this work paves the way toward more general-purpose,\nreal-time 3D understanding systems.\nOur method comes with some limitations. First, the na\u00efve causal modeling naturally suffers from\nerror accumulation and drifting [ 95]. Some inference strategies can be proposed to alleviate this issue.\nSecond, currently STREAM 3Ris still a regression model with deterministic outputs. Extending it\nfurther into an autoregressive generative model [ 25,95] shall further unlock a series of downstream\napplications. Finally, since STREAM 3Rfollows a similar design of modern LLMs, more training\ntechniques like MLA [ 65] can be introduced to further boost the training efficiency and performance.\n10\n\n--- Page 11 ---\nReferences\n[1]Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In CVPR , pages\n4104\u20134113, 2016.\n[2]Johannes Lutz Sch\u00f6nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view\nselection for unstructured multi-view stereo. In ECCV , 2016.\n[3]Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf:\nFast generalizable radiance field reconstruction from multi-view stereo. In ICCV , pages 14124\u201314133,\n2021.\n[4]Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The kitti\ndataset. IJRR , 2013.\n[5]Yufeng Zheng, Wang Yifan, Gordon Wetzstein, Michael J. Black, and Otmar Hilliges. Pointavatar:\nDeformable point-based head avatars from videos. In CVPR , 2023.\n[6]Yushi Lan, Feitong Tan, Di Qiu, Qiangeng Xu, Kyle Genova, Zeng Huang, Sean Fanello, Rohit Pandey,\nThomas Funkhouser, Chen Change Loy, and Yinda Zhang. Gaussian3diff: 3d gaussian diffusion for 3d\nfull head synthesis and editing. In ECCV , 2024.\n[7]Muhammad Zubair Irshad, Mauro Comi, Yen-Chen Lin, Nick Heppert, Abhinav Valada, Rares Ambrus,\nZsolt Kira, and Jonathan Tremblay. Neural fields in robotics: A survey, 2024.\n[8]Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth inference for unstructured\nmulti-view stereo. ECCV , 2018.\n[9]Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for\nhigh-resolution multi-view stereo depth inference. CVPR , 2019.\n[10] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric\n3d vision made easy. In CVPR , pages 20697\u201320709, 2024.\n[11] Vincent Leroy, Yohann Cabon, and Jerome Revaud. Grounding image matching in 3d with mast3r, 2024.\n[12] Jianing Yang, Alexander Sax, Kevin J. Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska\nMeier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In\nCVPR , June 2025.\n[13] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny.\nVggt: Visual geometry grounded transformer. In CVPR , 2025.\n[14] Andrew J Davison, Ian D Reid, Nicholas D Molton, and Olivier Stasse. Monoslam: Real-time single\ncamera slam. TPAMI , 29(6):1052\u20131067, 2007.\n[15] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint\narXiv:2408.16061 , 2024.\n[16] Ho Kei Cheng and Alexander G. Schwing. XMem: Long-term video object segmentation with an\natkinson-shiffrin memory model. In ECCV , 2022.\n[17] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei A Efros, and Angjoo Kanazawa. Continuous\n3d perception model with persistent state. arXiv preprint arXiv:2501.12387 , 2025.\n[18] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization, 2015.\n[19] Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In ICLR , 2024.\n[20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. Llama: Open and efficient foundation language models, 2023.\n[21] Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and Alexandre\nD\u00e9fossez. Simple and controllable music generation. Advances in Neural Information Processing Systems ,\n36, 2023.\n[22] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding, 2019.\n11\n\n--- Page 12 ---\n[23] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor,\nTroy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as\nworld simulators. 2024.\n[24] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens\nWinter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language\nmodels are few-shot learners, 2020.\n[25] Boyuan Chen, Diego Mart\u00ed Mons\u00f3, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann.\nDiffusion forcing: Next-token prediction meets full-sequence diffusion. NeurIPS , 37:24081\u201324125, 2025.\n[26] Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard\nLavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e\nLacroix, and William El Sayed. Mistral 7b, 2023.\n[27] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision . Cambridge university\npress, 2003.\n[28] Chengzhou Tang and Ping Tan. BA-Net: Dense bundle adjustment network. arXiv preprint\narXiv:1806.04807 , 2018.\n[29] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D Tardos. ORB-SLAM: a versatile and accurate\nmonocular SLAM system. IEEE Transactions on Robotics , 31(5):1147\u20131163, 2015.\n[30] Zachary Teed and Jia Deng. DROID-SLAM: Deep visual SLAM for monocular, stereo, and RGB-D\ncameras. NeurIPS , pages 16558\u201316569, 2021.\n[31] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren\nNg. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV , 2020.\n[32] Kai Zhang, Gernot Riegler, Noah Snavely, and Vladlen Koltun. Nerf++: Analyzing and improving neural\nradiance fields. arXiv preprint arXiv:2010.07492 , 2020.\n[33] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS:\nLearning neural implicit surfaces by volume rendering for multi-view reconstruction. arXiv preprint\narXiv:2106.10689 , 2021.\n[34] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3D gaussian splatting for\nreal-time radiance field rendering. ACM Transactions on Graphics , 42(4):1\u201314, 2023.\n[35] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting\nfor geometrically accurate radiance fields. In SIGGRAPH 2024 Conference Papers . Association for\nComputing Machinery, 2024.\n[36] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu,\nYawen Lu, et al. Dl3dv-10k: A large-scale scene dataset for deep learning-based 3d vision. In CVPR ,\npages 22160\u201322169, 2024.\n[37] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David\nNovotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruc-\ntion. In ICCV , 2021.\n[38] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth\nanything: Unleashing the power of large-scale unlabeled data. In CVPR , 2024.\n[39] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad\nSchindler. Repurposing diffusion-based image generators for monocular depth estimation. In CVPR ,\npages 9492\u20139502, 2024.\n[40] Wenbo Hu, Xiangjun Gao, Xiaoyu Li, Sijie Zhao, Xiaodong Cun, Yong Zhang, Long Quan, and Ying\nShan. Depthcrafter: Generating consistent long depth sequences for open-world videos. In CVPR , 2025.\n[41] Zhengqi Li, Richard Tucker, Forrester Cole, Qianqian Wang, Linyi Jin, Vickie Ye, Angjoo Kanazawa,\nAleksander Holynski, and Noah Snavely. MegaSaM: Accurate, fast and robust structure and motion from\ncasual dynamic videos. arXiv preprint , 2024.\n12\n\n--- Page 13 ---\n[42] Qianqian Wang, Vickie Ye, Hang Gao, Weijia Zeng, Jake Austin, Zhengqi Li, and Angjoo Kanazawa.\nShape of motion: 4d reconstruction from a single video. 2024.\n[43] Jianyuan Wang, Nikita Karaev, Christian Rupprecht, and David Novotny. Vggsfm: Visual geometry\ngrounded deep structure from motion. In CVPR , pages 21686\u201321697, 2024.\n[44] Wei Yin, Chi Zhang, Hao Chen, Zhipeng Cai, Gang Yu, Kaixuan Wang, Xiaozhi Chen, and Chunhua\nShen. Metric3d: Towards zero-shot metric 3d prediction from a single image. In CVPR , pages 9043\u20139053,\n2023.\n[45] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen\nCham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. arXiv\npreprint arXiv:2403.14627 , 2024.\n[46] David Charatan, Sizhe Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats\nfrom image pairs for scalable generalizable 3d reconstruction. In CVPR , 2024.\n[47] Jiale Xu, Shenghua Gao, and Ying Shan. Freesplatter: Pose-free gaussian splatting for sparse-view 3d\nreconstruction. arXiv preprint arXiv:2412.09573 , 2024.\n[48] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast single-view\n3D reconstruction. In arXiv , 2023.\n[49] Junyi Zhang, Charles Herrmann, Junhwa Hur, Varun Jampani, Trevor Darrell, Forrester Cole, Deqing\nSun, and Ming-Hsuan Yang. Monst3r: A simple approach for estimating geometry in the presence of\nmotion. arXiv preprint arxiv:2410.03825 , 2024.\n[50] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm:\nLarge reconstruction model for 3d gaussian splatting. ECCV , 2024.\n[51] Vincent Sitzmann, Justus Thies, Felix Heide, Matthias NieBner, Gordon Wetzstein, and Michael Zollhofer.\nDeepV oxels: Learning persistent 3D feature embeddings. In CVPR , pages 2432\u20132441. IEEE.\n[52] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh R-CNN. In ICCV , pages 9785\u20139795, 2019.\n[53] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf:\nLearning continuous signed distance functions for shape representation. In CVPR , pages 165\u2013174, 2019.\n[54] Riku Murai, Eric Dexheimer, and Andrew J. Davison. MASt3R-SLAM: Real-time dense SLAM with 3D\nreconstruction priors. arXiv preprint , 2024.\n[55] Yuzheng Liu, Siyan Dong, Shuzhe Wang, Yingda Yin, Yanchao Yang, Qingnan Fan, and Baoquan\nChen. Slam3r: Real-time dense scene reconstruction from monocular rgb videos. arXiv preprint\narXiv:2412.09401 , 2024.\n[56] Botao Ye, Sifei Liu, Haofei Xu, Li Xueting, Marc Pollefeys, Ming-Hsuan Yang, and Peng Songyou. No\npose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. In ICLR , 2025.\n[57] Jiahui Lei, Yijia Weng, Adam Harley, Leonidas Guibas, and Kostas Daniilidis. Mosca: Dynamic gaussian\nfusion from casual videos via 4d motion scaffolds. arXiv preprint arXiv:2405.17421 , 2024.\n[58] Wen-Hsuan Chu, Lei Ke, and Katerina Fragkiadaki. Dreamscene4d: Dynamic multi-object scene\ngeneration from monocular videos. NeurIPS , 2024.\n[59] Johannes Kopf, Xuejian Rong, and Jia-Bin Huang. Robust consistent video depth estimation. In CVPR ,\n2021.\n[60] Zihan Zhu, Songyou Peng, Viktor Larsson, Zhaopeng Cui, Martin R Oswald, Andreas Geiger, and Marc\nPollefeys. Nicer-slam: Neural implicit scene encoding for rgb slam. In 3DV, March 2024.\n[61] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3D-r2n2: A unified\napproach for single and multi-view 3D object reconstruction. In ECCV , volume 9912 of Lecture Notes in\nComputer Science , pages 628\u2013644. Springer, 2016.\n[62] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. PixelNeRF: Neural radiance fields from\none or few images. In CVPR , 2021.\n[63] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P. Srinivasan, Howard Zhou, Jonathan T. Barron,\nRicardo Martin-Brualla, Noah Snavely, and Thomas A. Funkhouser. IBRNet: Learning Multi-View\nImage-Based Rendering. In CVPR , 2021.\n13\n\n--- Page 14 ---\n[64] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR ,\n2021.\n[65] DeepSeek-AI, Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi\nDengr, Chong Ruan, Damai Dai, Daya Guo, Dejian Yang, Deli Chen, Dongjie Ji, Erhang Li, Fangyun\nLin, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Hanwei Xu, Hao Yang, Haowei\nZhang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Li, Hui Qu, J. L. Cai, Jian Liang, Jianzhong Guo,\nJiaqi Ni, Jiashi Li, Jin Chen, Jingyang Yuan, Junjie Qiu, Junxiao Song, Kai Dong, Kaige Gao, Kang\nGuan, Lean Wang, Lecong Zhang, Lei Xu, Leyi Xia, Liang Zhao, Liyue Zhang, Meng Li, Miaojun Wang,\nMingchuan Zhang, Minghua Zhang, Minghui Tang, Mingming Li, Ning Tian, Panpan Huang, Peiyi Wang,\nPeng Zhang, Qihao Zhu, Qinyu Chen, Qiushi Du, R. J. Chen, R. L. Jin, Ruiqi Ge, Ruizhe Pan, Runxin\nXu, Ruyi Chen, S. S. Li, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shaoqing Wu, Shengfeng Ye,\nShirong Ma, Shiyu Wang, Shuang Zhou, Shuiping Yu, Shunfeng Zhou, Size Zheng, T. Wang, Tian Pei,\nTian Yuan, Tianyu Sun, W. L. Xiao, Wangding Zeng, Wei An, Wen Liu, Wenfeng Liang, Wenjun Gao,\nWentao Zhang, X. Q. Li, Xiangyue Jin, Xianzu Wang, Xiao Bi, Xiaodong Liu, Xiaohan Wang, Xiaojin\nShen, Xiaokang Chen, Xiaosha Chen, Xiaotao Nie, Xiaowen Sun, Xiaoxiang Wang, Xin Liu, Xin Xie,\nXingkai Yu, Xinnan Song, Xinyi Zhou, Xinyu Yang, Xuan Lu, Xuecheng Su, Y . Wu, Y . K. Li, Y . X. Wei,\nY . X. Zhu, Yanhong Xu, Yanping Huang, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Li, Yaohui Wang,\nYi Zheng, Yichao Zhang, Yiliang Xiong, Yilong Zhao, Ying He, Ying Tang, Yishi Piao, Yixin Dong,\nYixuan Tan, Yiyuan Liu, Yongji Wang, Yongqiang Guo, Yuchen Zhu, Yuduan Wang, Yuheng Zou, Yukun\nZha, Yunxian Ma, Yuting Yan, Yuxiang You, Yuxuan Liu, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu,\nZhen Huang, Zhen Zhang, Zhenda Xie, Zhewen Hao, Zhihong Shao, Zhiniu Wen, Zhipeng Xu, Zhongyu\nZhang, Zhuoshu Li, Zihan Wang, Zihui Gu, Zilin Li, and Ziwei Xie. Deepseek-v2: A strong, economical,\nand efficient mixture-of-experts language model, 2024.\n[66] Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. ArXiv\npreprint , 2021.\n[67] Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, and Andrea Vedaldi. Geo4d: Leveraging video\ngenerators for geometric 4d scene reconstruction, 2025.\n[68] Yunze Liu, Yun Liu, Che Jiang, Kangbo Lyu, Weikang Wan, Hao Shen, Boqiang Liang, Zhoujie Fu,\nHe Wang, and Li Yi. Hoi4d: A 4d egocentric dataset for category-level human-object interaction. In\nCVPR , pages 21013\u201321022, June 2022.\n[69] Ruicheng Wang, Sicheng Xu, Cassie Dai, Jianfeng Xiang, Yu Deng, Xin Tong, and Jiaolong Yang.\nMoge: Unlocking accurate monocular geometry estimation for open-domain images with optimal training\nsupervision, 2024.\n[70] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nie\u00dfner, and Angela Dai. Scannet++: A high-fidelity\ndataset of 3d indoor scenes. In ICCV , pages 12\u201322, 2023.\n[71] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner.\nScannet: Richly-annotated 3d reconstructions of indoor scenes. In CVPR , 2017.\n[72] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit Kumar, Miguel Angel Bautista, Nathan Paczan,\nRuss Webb, and Joshua M. Susskind. Hypersim: A photorealistic synthetic dataset for holistic indoor\nscene understanding. In ICCV , 2021.\n[73] Nikita Karaev, Ignacio Rocco, Benjamin Graham, Natalia Neverova, Andrea Vedaldi, and Christian\nRupprecht. Dynamicstereo: Consistent dynamic depth from stereo videos. CVPR , 2023.\n[74] Yao Yao, Zixin Luo, Shiwei Li, Jingyang Zhang, Yufan Ren, Lei Zhou, Tian Fang, and Long Quan.\nBlendedmvs: A large-scale dataset for generalized multi-view stereo networks. CVPR , 2020.\n[75] Xiaqing Pan, Nicholas Charron, Yongqian Yang, Scott Peters, Thomas Whelan, Chen Kong, Omkar\nParkhi, Richard Newcombe, and Yuheng (Carl) Ren. Aria digital twin: A new benchmark dataset for\negocentric 3d machine perception. In ICCV , pages 20133\u201320143, October 2023.\n[76] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish\nKapoor, and Sebastian Scherer. Tartanair: A dataset to push the limits of visual slam. In IROS , 2020.\n[77] Eduardo Arnold, Jamie Wynn, Sara Vicente, Guillermo Garcia-Hernando, \u00c1ron Monszpart, Victor Adrian\nPrisacariu, Daniyar Turmukhambetov, and Eric Brachmann. Map-free visual relocalization: Metric pose\nrelative to a single image. In ECCV , 2022.\n14\n\n--- Page 15 ---\n[78] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos.\nInICCV , pages 2041\u20132050, 2018.\n[79] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer,\nBrandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. Arkitscenes: A diverse real-world dataset\nfor 3d indoor scene understanding using mobile rgb-d data, 2022.\n[80] Weinzaepfel, Philippe and Leroy, Vincent and Lucas, Thomas and Br\u00e9gier, Romain and Cabon, Yohann\nand Arora, Vaibhav and Antsfeld, Leonid and Chidlovskii, Boris and Csurka, Gabriela and Revaud J\u00e9r\u00f4me.\nCroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion. In NeurIPS , 2022.\n[81] Philippe Weinzaepfel, Thomas Lucas, Vincent Leroy, Yohann Cabon, Vaibhav Arora, Romain Br\u00e9gier,\nGabriela Csurka, Leonid Antsfeld, Boris Chidlovskii, and J\u00e9r\u00f4me Revaud. CroCo v2: Improved Cross-\nview Completion Pre-training for Stereo Matching and Optical Flow. In ICCV , 2023.\n[82] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer,\nAndreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision\ntransformers to 22 billion parameters. In ICML , 2023.\n[83] Maxime Oquab, Timoth\u00e9e Darcet, Theo Moutakanni, Huy V . V o, Marc Szafraniec, Vasil Khalidov, Pierre\nFernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu,\nVasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel\nSynnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski.\nDINOv2: Learning robust visual features without supervision, 2023.\n[84] Dong Zhuo, Wenzhao Zheng, Jiahe Guo, Yuqi Wu, Jie Zhou, and Jiwen Lu. Streaming 4d visual geometry\ntransformer. arXiv preprint arXiv:2507.11539 , 2025.\n[85] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. A naturalistic open source movie for optical\nflow evaluation. In A. Fitzgibbon et al. (Eds.), editor, ECCV , Part IV , LNCS 7577, pages 611\u2013625.\nSpringer-Verlag, October 2012.\n[86] E. Palazzolo, J. Behley, P. Lottes, P. Gigu\u00e8re, and C. Stachniss. ReFusion: 3D Reconstruction in Dynamic\nEnvironments for RGB-D Cameras Exploiting Residuals. arXiv , 2019.\n[87] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support\ninference from rgbd images. In ECCV , pages 746\u2013760. Springer-Verlag Berlin, October 2012.\n[88] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth\nanything: Unleashing the power of large-scale unlabeled data. In CVPR , 2024.\n[89] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field transforms for optical flow, 2020.\n[90] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon.\nScene coordinate regression forests for camera relocalization in rgb-d images. In CVPR , June 2013.\n[91] Dejan Azinovi \u00b4c, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, and Justus Thies. Neural\nrgb-d surface reconstruction. In CVPR , pages 6290\u20136301, June 2022.\n[92] J\u00fcrgen Sturm, Nikolas Engelhard, Felix Endres, Wolfram Burgard, and Daniel Cremers. A benchmark for\nthe evaluation of rgb-d slam systems. In IEEE/RSJ International Conference on Intelligent Robots and\nSystems , pages 573\u2013580, 2012.\n[93] Wang Zhao, Shaohui Liu, Hengkai Guo, Wenping Wang, and Yong-Jin Liu. Particlesfm: Exploiting dense\npoint trajectories for localizing moving cameras in the wild. In ECCV , 2022.\n[94] Zhoutong Zhang, Forrester Cole, Zhengqi Li, Michael Rubinstein, Noah Snavely, and William T Freeman.\nStructure and motion from casual videos. In ECCV , pages 20\u201337. Springer, 2022.\n[95] Lvmin Zhang and Maneesh Agrawala. Packing input frame contexts in next-frame prediction models for\nvideo generation. Arxiv , 2025.\n[96] Hongchi Xia, Yang Fu, Sifei Liu, and Xiaolong Wang. Rgbd objects in the wild: Scaling real-world 3d\nobject learning from rgb-d videos, 2024.\n[97] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James\nGuo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao,\nAleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens,\nZhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open\ndataset. In CVPR , June 2020.\n15\n\n--- Page 16 ---\n[98] Michael J. Black, Priyanka Patel, Joachim Tesch, and Jinlong Yang. BEDLAM: A synthetic dataset of\nbodies exhibiting detailed lifelike animated motion. In CVPR , pages 8726\u20138737, June 2023.\n[99] Simon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from a single image. ACM\nTransactions on Graphics , 38(6):184:1\u2013184:15, 2019.\n[100] Qiang Wang, Shizhen Zheng, Qingsong Yan, Fei Deng, Kaiyong Zhao, and Xiaowen Chu. Irs: A large\nnaturalistic indoor robotics stereo dataset to train deep models for disparity and surface normal estimation,\n2021.\n[101] Anastasiia Kornilova, Marsel Faizullin, Konstantin Pakulev, Andrey Sadkov, Denis Kukushkin, Azat\nAkhmetyanov, Timur Akhtyamov, Hekmat Taherinejad, and Gonzalo Ferrer. Smartportraits: Depth\npowered handheld smartphone dataset of human portraits for state estimation, reconstruction and synthesis,\n2022.\n[102] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao.\nDepth anything V2. arXiv preprint arXiv:2406.09414 , 2024.\n[103] Yiran Wang, Min Shi, Jiaqi Li, Zihao Huang, Zhiguo Cao, Jianming Zhang, Ke Xian, and Guosheng Lin.\nNeural video depth stabilizer. In ICCV , pages 9466\u20139476, October 2023.\n[104] Jiahao Shao, Yuanbo Yang, Hongyu Zhou, Youmin Zhang, Yujun Shen, Vitor Guizilini, Yue Wang, Matteo\nPoggi, and Yiyi Liao. Learning temporally consistent video depth from video diffusion priors, 2024.\n[105] Ioan Andrei B\u00e2rsan, Peidong Liu, Marc Pollefeys, and Andreas Geiger. Robust dense mapping for\nlarge-scale dynamic environments. In ICRA , pages 7510\u20137517, 2018.\n16\n\n--- Page 17 ---\nA Dataset Details\nWe train our model on 29datasets that contains a diverse range of scene types, including static and dy-\nnamic scene and objects. Specifically, we mainly follow the data splits of CUT3R [ 17], and the main\n15datasets with highest sampling ratio are: Co3Dv2 [ 37], ScanNet++ [ 70], ScanNet [ 71], Hyper-\nSim [ 72], Dynamic Replica [ 73], DL3DV [ 36], BlendedMVS [ 74], Aria Synthetic Environments [ 75],\nTartanAir [ 76], MapFree [ 77], MegaDepth [ 78], WildRGBD [ 96], Waymo [ 97], Bedlam [ 98], and\nARKitScenes [ 79]. We do not include 3D Ken Burns [ 99], IRS [ 100], and SmartPortraits [ 101] for\ntraining since these datasets are either single view or fail to download successfully. We adapt the\nofficial scripts provided by CUT3R [ 17], DUSt3R [ 10], and Spann3R [ 15] for dataset processing. For\ntraining STREAM 3R\u03b2, we remove all the single-view datasets as in VGG-T, leaving 19datasets for\ntraining. We did not find performance degradation when removing the single-view datasets. Please\nrefer to the Tab. 6 of the CUT3R for more dataset details.\nB More Implementation details\nMore Training Details. Our method conducts end-to-end training on all datasets on a hybrid of 12\ndifferent resolutions, ranging from 224\u00d7224to512\u00d7384. Data augmentation side, we perform\nsequence-level color jittering by applying the same color jitter across all frames in a sequence.\nNetwork Architecture Details. We follow DUSt3R and use the CroCoNet [ 81] pre-trained ViT\nfor the encoder and decoder design. We directly use the DPT [ 66] head for Head global andHead local\nimplementation. We apply RoPE to the query and key feature before each attention operation for the\nViT encoder, but ignore it for the ViT decoder to generalize to an arbitrary number of input views.\nFor ablation studies, we train our model on the same datasets but at resolution 224\u00d7224.\nFor the sliding window attention version STREAM 3R\u03b2-W[5], we always include the tokens of the\nfirst frame to keep the canonical coordinate space unchanged. We set window size W = 5since it\ntrades off performance and speed, and other window size also stably works. For the full attention\nversion STREAM 3R\u03b2-FA, we directly use the causally trained model STREAM 3R\u03b2and remove the\ncausal mask in the SelfAttn . This is similar to the \u201crevisit\u201d operation in CUT3R.\nC More Comparisons\nVideo Depth Estimation. We further expand the video depth comparison in the main paper and\ninclude a wider range of baseline methods, including single-frame depth methods (Marigold [ 39]\nand DepthAnything-V2 [ 102]), video depth approaches (NVDS [ 103], DepthCrafter [ 40], and Chron-\noDepth [ 104]), and recent joint depth-and-pose estimation methods such as Robust-CVD [ 105],\nCausalSAM [ 94], DUSt3R [ 10], MASt3R [ 11], MonST3R [ 49], and Spann3R [ 15]. Extended results\nare shown in Tab. 7. STREAM 3Rconsistently outperforms its RNN-based counterpart CUT3R under\nthe per-sequence scale &shift setting, and even achieves state-of-the-art performance on the KITTI\ndataset\u2014while also being the fastest in terms of FPS.\n3D Reconstruction on NRGBD. We further include the comparison on NRGBD benchmark [ 91]\nin Tab. 8. Here, we also include the comparison with a concurrent work StreamVGGT [ 84], which\nfine-tunes VGG-T into streaming version similar to our method. We also include VGG-T[streaming],\nwhich indicates using VGG-T in the streaming setting by replace the full attention in VGG-T into\nthe causal attention. As can be seen, our method clearly outperforms all optimization-based and\nonline methods, including the official VGG-T model. Direct use of VGG-T in the streaming setting\nsubstantially degrades performance, underscoring the need for fine-tuning under causal constraints.\n17\n\n--- Page 18 ---\nTable 7: Video Depth Evaluation . We report scale&shift-invariant depth, scale-invariant depth and metric\ndepth accuracy on Sintel, Bonn, and KITTI datasets. Methods requiring global alignment are marked \u201cGA\u201d,\nwhile \u201cOptim\u201d and \u201cStream\u201d indicate Optimzation-based and Streamne methods, respectively. We also report\nthe FPS on KITTI dataset using 512 \u00d7144 image resolution for all methods, except Spann3R which Stream\nsupports 224 \u00d7224 inputs.\nSintel BONN KITTI\nAlignment Method Type Abs Rel \u2193\u03b4<1.25\u2191Abs Rel \u2193\u03b4<1.25\u2191Abs Rel \u2193\u03b4<1.25\u2191FPS\nPer-sequence\nscale & shiftMarigold [39] Stream 0.532 51.5 0.091 93.1 0.149 79.6 <0.1\nDepth-Anything-V2 [102] Stream 0.367 55.4 0.106 92.1 0.140 80.4 3.13\nNVDS [103] Stream 0.408 48.3 0.167 76.6 0.253 58.8 -\nChronoDepth [104] Stream 0.687 48.6 0.100 91.1 0.167 75.9 1.89\nDepthCrafter [40] Stream 0.292 69.7 0.075 97.1 0.110 88.1 0.97\nRobust-CVD [59] Stream 0.703 47.8 - - - - -\nCasualSAM [94] Optim 0.387 54.7 0.169 73.7 0.246 62.2 -\nDUSt3R-GA [10] Optim 0.531 51.2 0.156 83.1 0.135 81.8 0.76\nMASt3R-GA [11] Optim 0.327 59.4 0.167 78.5 0.137 83.6 0.31\nMonST3R-GA [49] Optim 0.333 59.0 0.066 96.4 0.157 73.8 0.35\nSpann3R [15] Stream 0.508 50.8 0.157 82.1 0.207 73.0 13.55\nCUT3R [17] Stream 0.540 55.7 0.074 94.5 0.106 88.7 16.58\nSTREAM 3R Stream 0.356 58.6 0.068 95.7 0.099 91.0 23.48\nPer-sequence scaleDUSt3R-GA [10] Optim 0.656 45.2 0.155 83.3 0.144 81.3 0.76\nMASt3R-GA [11] Optim 0.641 43.9 0.252 70.1 0.183 74.5 0.31\nMonST3R-GA [49] Optim 0.378 55.8 0.067 96.3 0.168 74.4 0.35\nSpann3R [15] Stream 0.622 42.6 0.144 81.3 0.198 73.7 13.55\nFast3R [12] FA 0.653 44.9 0.193 77.5 0.140 83.4 47.23\nCUT3R [17] Stream 0.421 47.9 0.078 93.7 0.118 88.1 16.58\nSTREAM 3R\u03b1Stream 0.478 51.1 0.075 94.1 0.116 89.6 23.48\nMetric scaleMASt3R-GA [11] Optim 1.022 14.3 0.272 70.6 0.467 15.2 0.31\nCUT3R Stream 1.029 23.8 0.103 88.5 0.122 85.5 16.58\nSTREAM 3R\u03b1Stream 1.041 21.0 0.084 94.4 0.234 57.6 23.48\nTable 8: 3D Reconstruction Comparison on NRGBD [ 91].Our proposed method consistently\nachieves superior performance compared to optimization-based (Optim), streaming-based (Stream),\nand even full attention (FA) methods.\nMethod TypeAcc\u2193 Comp\u2193 NC\u2191\nMean Med. Mean Med. Mean Med.\nVGG-T [13] FA 0.073 0.018 0.077 0.021 0.910 0.990\nDUSt3R-GA [10] Optim 0.144 0.019 0.154 0.018 0.870 0.982\nMASt3R-GA [11] Optim 0.085 0.033 0.063 0.028 0.794 0.928\nMonST3R-GA [49] Optim 0.272 0.114 0.287 0.110 0.758 0.843\nSpann3R [15] Stream 0.416 0.323 0.417 0.285 0.684 0.789\nCUT3R [17] Stream 0.099 0.031 0.076 0.026 0.837 0.971\nStreamVGGT [84] Stream 0.084 0.044 0.074 0.041 0.861 0.986\nVGG-T [Streaming] [13] Stream 0.219 0.102 0.212 0.105 0.797 0.936\nSTREAM 3R\u03b2Stream 0.057 0.014 0.028 0.013 0.910 0.993\n18",
  "project_dir": "artifacts/projects/enhanced_cs.CV_2508.10893v1_STream3R_Scalable_Sequential_3D_Reconstruction_wi",
  "communication_dir": "artifacts/projects/enhanced_cs.CV_2508.10893v1_STream3R_Scalable_Sequential_3D_Reconstruction_wi/.agent_comm",
  "assigned_at": "2025-08-17T20:52:58.708898",
  "status": "assigned"
}